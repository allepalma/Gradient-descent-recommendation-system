{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data parsing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we retrieve and parse the data concerning both ratings, movies and users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data parsing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the data and retrieve the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Data Frame containing the ratings.\n",
    "data = np.genfromtxt('./ratings.dat',\n",
    "                     delimiter='::', usecols = (0, 1, 2), dtype = int)\n",
    "\n",
    "#Convert to a data frame and rename the columns\n",
    "ratings = pd.DataFrame(data)\n",
    "ratings.columns = ['user', 'movie', 'rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the training userxmovie matrix\n",
    "rating_mat = ratings.pivot(index='user', columns='movie', values='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will work with numpy arrays since faster. \n",
    "ratings_np = np.matrix(ratings)\n",
    "rating_mat_np = np.array(rating_mat)\n",
    "\n",
    "#ratings_np represents the data frame of ratings with information encoded as <userID, moviesID, rating>. \n",
    "#rating_mat_np is the sparse matrix of ratings we use for the optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the Gradient Descent method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the gradient descent method described by Tikk et al. in the paper named \"On the Gravity Recommendation System\". The procedure is based on the approximation of the sparse utility matrix by mean of two lower rank matrices (U and M) which are optimized towards the implementation of the so-called gravity recommendation system. We choose to initialize the matrices U and M such that their dot product yields a matrix with as elements the mean of all values of the utility matrix we want to approximate, subjected to random pertubations. U and V are dimensional reductions of the of the utility matrix representing every user and item as vectors of size $K$.\n",
    "\n",
    "Given an $m*n$ data matrix $X$, the system starts from the hypothesis that the rating entry $X_{ij}$ is well approximated by the dot product of the vectors $u_i$ and $m_j$, representing respectively the $i^{th}$ row and $j^{th}$ column of $U$ and $M$. Therefore, our prediction for the matrix $X$ is represented by the matrix product of $U$ and $M$.\n",
    "\n",
    "The algorithm will iterate across all available ratings from our dataset and optimize the value of the weights depending on the gradient of the error measure which, in our case, is represented by the squared difference between the observed and the predicted rating values. At each step, the value of the weights for a certain user and item will be updated with the gradient of the error function downsized by a learning rate positive constant. Ideally, the algroithm would cycle until convergence. For time necessities, we will limit our number of iterations to a maximum of 75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(ratings, n_users, n_movies, K):\n",
    "    '''\n",
    "    Given a rating data frame \"ratings\", the number of users and the number of movies and a constant K, the algorithm \n",
    "    initializes two matrices U and M of dimensions n_users*K and K*n_movies\n",
    "    '''\n",
    "    nan_mean = np.mean(ratings['rating'])\n",
    "    initialize = np.sqrt(nan_mean/K)\n",
    "    U = (np.repeat(initialize, K*n_users) + np.random.normal(0,1,K*n_users)).reshape((n_users, K))\n",
    "    M = (np.repeat(initialize, K*n_movies) + np.random.normal(0,1,K*n_movies)).reshape((K, n_movies))\n",
    "    return U,M\n",
    "\n",
    "def RMSE(ratings, X, UM):\n",
    "    '''\n",
    "    RMSE computes the root mean squared error of two matrices, X and UM. Ratings is an array containing the list of\n",
    "    ratings recorded as <userID, itemID, rating>.\n",
    "    '''\n",
    "    diff = X - UM\n",
    "    n_non_missing = len(ratings)\n",
    "    rmse = np.sqrt(np.nansum(diff**2)/n_non_missing)\n",
    "    return rmse\n",
    "\n",
    "def GRS(ratings, X, U, M,  row_names, col_names, iter = 75, lamb = 0.05, lrate = 0.005):\n",
    "    '''\n",
    "    Given an array of the form <userID, movieID, ratings>, a sparse matrix X and the two initialized matrices U and V, \n",
    "    implement the gravity recommendation system algorithm. The dictionaries row_names and col_names encode for the mapping\n",
    "    between the ID of a user or item and the row/column index of the matrix X.\n",
    "    '''\n",
    "    #In res, we append the result of the RMSE error for each iteration for later plotting.\n",
    "    res = []\n",
    "    #Compute the first RMSE. \n",
    "    RMSE_new = RMSE(ratings, X, U.dot(M))\n",
    "    #Initialize the RMSE_old to infinity.\n",
    "    RMSE_old = np.inf\n",
    "    count = 1\n",
    "    #The algorithm will stop after a default number of iterations or when the old and new RMSEs converge. \n",
    "    while count <= iter and RMSE_old != RMSE_new:\n",
    "        for rat in ratings:\n",
    "            (i,j) = row_names[rat[0,0]], col_names[rat[0,1]]\n",
    "            e = X[i,j] - U[i,:].dot(M[:,j])\n",
    "            U_copy = copy.copy(U[i,:]) #Copy the object U to avoid side effects as suggested in th Simon Funk's blog.\n",
    "            U[i,:] += lrate*(2*e*M[:,j] - lamb*U[i,:])\n",
    "            M[:,j] += lrate*(2*e*U_copy - lamb*M[:,j])\n",
    "        RMSE_old = RMSE_new\n",
    "        RMSE_new = RMSE(ratings, X, U.dot(M))\n",
    "        res.append(RMSE_new)\n",
    "        count += 1\n",
    "    return U.dot(M), res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following script, we will estimate the test error of our approach via 5-fold cross-validation. Here are the parameters we used for producing the results:\n",
    "* The regularization factor $\\lambda$ is set equal to 0.05\n",
    "* The learning rate is set to 0.005\n",
    "* The value of K (hence, the number of features for each user and item) is equal to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test by cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement 5-fold cross validation\n",
    "\n",
    "#We will keep a partial error for plotting\n",
    "partial_error = []\n",
    "\n",
    "#The parameters \n",
    "K = 10\n",
    "iter = 75\n",
    "lamb = 0.05\n",
    "lrate = 0.005\n",
    "\n",
    "#Set a seed for reproducibility\n",
    "np.random.seed(888)\n",
    "\n",
    "#Generate the 5 folds for all rows of the utility matrix of input.\n",
    "nfolds = 5\n",
    "folds = np.array([x%nfolds for x in range(len(ratings))])\n",
    "np.random.shuffle(folds)\n",
    "\n",
    "\n",
    "#Set up the vectors of errors.\n",
    "errors_5folds_rmse_training = []\n",
    "errors_5folds_rmse_test = [] \n",
    "\n",
    "'''\n",
    "We will cycle through the possible folds (from 1 to 5) and leave one set of observations (test set) out of the training process.\n",
    "Then, we use it to perform predictions and compute the error. \n",
    "'''\n",
    "\n",
    "for i in range(5):\n",
    "    print('Start iteration '+ str(i))\n",
    "    #Fix the rows of the ratings Data Frame that will be the training set.\n",
    "    train = ratings.loc[folds!=i, :]\n",
    "    train_mat = train.pivot(index='user', columns='movie', values='rating')\n",
    "    train_np = np.matrix(train)\n",
    "    train_mat_np = np.array(train_mat)\n",
    "    \n",
    "    #Create lookup dictionary associating the indices of the numpy array to the\n",
    "    #respective movieID and userID.\n",
    "    col_names_train = {train_mat.columns[j]:j for j in range(len(train_mat.columns))}\n",
    "    row_names_train = {train_mat.index[j]:j for j in range(len(train_mat.index))} \n",
    "    \n",
    "    #Initialize U and M and run the prediction.\n",
    "    U, M = initialize(train, np.size(train_mat_np,0), np.size(train_mat_np,1), K)\n",
    "    pred, res = GRS(train_np, train_mat_np, U, M, row_names_train, col_names_train)\n",
    "    \n",
    "    #The global mean of the training matrix will be used as fallback rule movies or user ID's absent in the training set.\n",
    "    global_mean = np.mean(train['rating'])\n",
    "    \n",
    "    #Initialize test set\n",
    "    test =  ratings.loc[folds == i, :]\n",
    "    test_np = np.matrix(test)\n",
    "    test_pred = []\n",
    "    \n",
    "    #Create the prediction vector of the test set. If a user or item are not present\n",
    "    #in the training set, predict their rating as the gloal mean if the prediction matrix.\n",
    "    for val in test_np:\n",
    "        if val[0,0] in row_names_train and val[0,1] in col_names_train:\n",
    "            test_pred.append(pred[row_names_train[val[0,0]], col_names_train[val[0,1]]])\n",
    "        else:\n",
    "            test_pred.append(global_mean)\n",
    "    test_pred = np.array(test_pred)\n",
    "    test_obs = np.array(test['rating'])\n",
    "    \n",
    "    #Extra fallback rules to improve performance\n",
    "    test_pred = np.where(test_pred > 5, 5, test_pred)\n",
    "    test_pred = np.where(test_pred < 1, 1, test_pred)\n",
    "    \n",
    "    #Calculate and store the errors on the training set and test set.\n",
    "    errors_5folds_rmse_training.append(RMSE(train_np,train_mat_np,pred))\n",
    "    \n",
    "    #Calculate the error on the test set and store it.\n",
    "    test_RMSE = np.sqrt(np.sum((test_pred-test_obs)**2)/len(test_pred)) \n",
    "    errors_5folds_rmse_test.append(test_RMSE)\n",
    "    partial_error.append(res)   \n",
    "    \n",
    "    print('End of iteration', i)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now analyse our results computing the estimated runtime and the mean RMSE over the training set. We will also plot the convergence of the RMSE on the training set across the different folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average across the folds to compute the errors. \n",
    "cv_error_rmse_train = np.mean(errors_5folds_rmse_training)\n",
    "cv_error_rmse_test = np.mean(errors_5folds_rmse_test)\n",
    "\n",
    "print('The average rmse over the training set is: ', cv_error_rmse_train)\n",
    "print('The average rmse over the test set is: ', cv_error_rmse_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
